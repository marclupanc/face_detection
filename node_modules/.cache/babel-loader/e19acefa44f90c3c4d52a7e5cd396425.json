{"ast":null,"code":"// import {useRef, useEffect, useState} from \"react\";\n// import './stream.css'\n// import * as faceapi from 'face-api.js';import '../../face-api.min'\n// // import {faceapi} from \"../../face-api.min\";\n//\n//\n// const Stream = ({videoState, setVideoState}) => {\n//\n//   const videoRef = useRef();\n//   const [modelsLoaded, setModelsLoaded] = useState(false);\n//   const [captureVideo, setCaptureVideo] = useState(false);\n//\n//   useEffect(() => {\n//     const loadModels = async () => {\n//       const MODEL_URL = process.env.PUBLIC_URL + '../../models';\n//\n//       Promise.all([\n//         faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL),\n//         faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL),\n//         faceapi.nets.faceRecognitionNet.loadFromUri(MODEL_URL),\n//         faceapi.nets.faceExpressionNet.loadFromUri(MODEL_URL),\n//       ]).then(setModelsLoaded(true));\n//     }\n//     loadModels();\n//   }, []);\n//   return (\n//     <div>\n//\n//     </div>\n//   )\n// }\n// //   useEffect(() => {\n// //     navigator.mediaDevices.getUserMedia({ video: videoState }).then((stream) => {\n// //       videoRef.current.srcObject = stream;\n// //     });\n// //   }, [videoState]);\n// //\n// //   return (\n// //     <div>\n// //       <button onClick={setVideoState}>CLICK</button>\n// //       <div className=\"video-box center\">\n// //         <video ref={videoRef} autoPlay/>\n// //       </div>\n// //     </div>\n// //   );\n// // }\n//\n// export default Stream;\n//\n//","map":{"version":3,"names":[],"sources":["/Users/maraclupanciuc/Desktop/SmartBrain/face-detection/face_detection/src/components/Stream/Stream.jsx"],"sourcesContent":["// import {useRef, useEffect, useState} from \"react\";\n// import './stream.css'\n// import * as faceapi from 'face-api.js';import '../../face-api.min'\n// // import {faceapi} from \"../../face-api.min\";\n//\n//\n// const Stream = ({videoState, setVideoState}) => {\n//\n//   const videoRef = useRef();\n//   const [modelsLoaded, setModelsLoaded] = useState(false);\n//   const [captureVideo, setCaptureVideo] = useState(false);\n//\n//   useEffect(() => {\n//     const loadModels = async () => {\n//       const MODEL_URL = process.env.PUBLIC_URL + '../../models';\n//\n//       Promise.all([\n//         faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL),\n//         faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL),\n//         faceapi.nets.faceRecognitionNet.loadFromUri(MODEL_URL),\n//         faceapi.nets.faceExpressionNet.loadFromUri(MODEL_URL),\n//       ]).then(setModelsLoaded(true));\n//     }\n//     loadModels();\n//   }, []);\n//   return (\n//     <div>\n//\n//     </div>\n//   )\n// }\n// //   useEffect(() => {\n// //     navigator.mediaDevices.getUserMedia({ video: videoState }).then((stream) => {\n// //       videoRef.current.srcObject = stream;\n// //     });\n// //   }, [videoState]);\n// //\n// //   return (\n// //     <div>\n// //       <button onClick={setVideoState}>CLICK</button>\n// //       <div className=\"video-box center\">\n// //         <video ref={videoRef} autoPlay/>\n// //       </div>\n// //     </div>\n// //   );\n// // }\n//\n// export default Stream;\n//\n//\n"],"mappings":"AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA"},"metadata":{},"sourceType":"module"}